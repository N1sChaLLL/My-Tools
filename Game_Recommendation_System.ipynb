{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a4d836b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STEAM GAME RECOMMENDATION SYSTEM - DATA LOADING\n",
      "================================================================================\n",
      "\n",
      "Spark Version: 3.5.3\n",
      "\n",
      "[1/4] Games: 50,872 rows\n",
      "Schema:\n",
      "root\n",
      " |-- app_id: integer (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- date_release: date (nullable = true)\n",
      " |-- win: boolean (nullable = true)\n",
      " |-- mac: boolean (nullable = true)\n",
      " |-- linux: boolean (nullable = true)\n",
      " |-- rating: string (nullable = true)\n",
      " |-- positive_ratio: integer (nullable = true)\n",
      " |-- user_reviews: integer (nullable = true)\n",
      " |-- price_final: double (nullable = true)\n",
      " |-- price_original: double (nullable = true)\n",
      " |-- discount: double (nullable = true)\n",
      " |-- steam_deck: boolean (nullable = true)\n",
      "\n",
      "\n",
      "Sample:\n",
      "+------+---------------------------------+------------+----+-----+-----+-------------+--------------+------------+-----------+--------------+--------+----------+\n",
      "|app_id|title                            |date_release|win |mac  |linux|rating       |positive_ratio|user_reviews|price_final|price_original|discount|steam_deck|\n",
      "+------+---------------------------------+------------+----+-----+-----+-------------+--------------+------------+-----------+--------------+--------+----------+\n",
      "|13500 |Prince of Persia: Warrior Within™|2008-11-21  |true|false|false|Very Positive|84            |2199        |9.99       |9.99          |0.0     |true      |\n",
      "|22364 |BRINK: Agents of Change          |2011-08-03  |true|false|false|Positive     |85            |21          |2.99       |2.99          |0.0     |true      |\n",
      "|113020|Monaco: What's Yours Is Mine     |2013-04-24  |true|true |true |Very Positive|92            |3722        |14.99      |14.99         |0.0     |true      |\n",
      "+------+---------------------------------+------------+----+-----+-----+-------------+--------------+------------+-----------+--------------+--------+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "\n",
      "[2/4] Users: 14,306,064 rows\n",
      "Schema:\n",
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- products: integer (nullable = true)\n",
      " |-- reviews: integer (nullable = true)\n",
      "\n",
      "\n",
      "Sample:\n",
      "+--------+--------+-------+\n",
      "|user_id |products|reviews|\n",
      "+--------+--------+-------+\n",
      "|7360263 |359     |0      |\n",
      "|14020781|156     |1      |\n",
      "|8762579 |329     |4      |\n",
      "+--------+--------+-------+\n",
      "only showing top 3 rows\n",
      "\n",
      "\n",
      "[3/4] Recommendations: 41,154,794 rows\n",
      "Schema:\n",
      "root\n",
      " |-- app_id: integer (nullable = true)\n",
      " |-- helpful: integer (nullable = true)\n",
      " |-- funny: integer (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- is_recommended: boolean (nullable = true)\n",
      " |-- hours: double (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- review_id: integer (nullable = true)\n",
      "\n",
      "\n",
      "Sample:\n",
      "+-------+-------+-----+----------+--------------+-----+-------+---------+\n",
      "|app_id |helpful|funny|date      |is_recommended|hours|user_id|review_id|\n",
      "+-------+-------+-----+----------+--------------+-----+-------+---------+\n",
      "|975370 |0      |0    |2022-12-12|true          |36.3 |51580  |0        |\n",
      "|304390 |4      |0    |2017-02-17|false         |11.5 |2586   |1        |\n",
      "|1085660|2      |0    |2019-11-17|true          |336.5|253880 |2        |\n",
      "+-------+-------+-----+----------+--------------+-----+-------+---------+\n",
      "only showing top 3 rows\n",
      "\n",
      "\n",
      "[4/4] Games Metadata: 1 rows\n",
      "Schema:\n",
      "root\n",
      " |-- app_id: long (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- tags: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "\n",
      "Sample:\n",
      "+------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|app_id|description                                                                                                                                                                                                                                                                                                                      |tags                                                                                                                                                                                                                                       |\n",
      "+------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|13500 |Enter the dark underworld of Prince of Persia Warrior Within, the sword-slashing sequel to the critically acclaimed Prince of Persia: The Sands of Time™. Hunted by Dahaka, an immortal incarnation of Fate seeking divine retribution, the Prince embarks upon a path of both carnage and mystery to defy his preordained death.|[Action, Adventure, Parkour, Third Person, Great Soundtrack, Singleplayer, Platformer, Time Travel, Atmospheric, Classic, Hack and Slash, Time Manipulation, Gore, Fantasy, Story Rich, Dark, Open World, Controller, Dark Fantasy, Puzzle]|\n",
      "+------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "\n",
      "================================================================================\n",
      "DATA QUALITY CHECK\n",
      "================================================================================\n",
      "\n",
      "Games Missing Values:\n",
      "+------+-----+------------+---+---+-----+------+--------------+------------+-----------+--------------+--------+----------+\n",
      "|app_id|title|date_release|win|mac|linux|rating|positive_ratio|user_reviews|price_final|price_original|discount|steam_deck|\n",
      "+------+-----+------------+---+---+-----+------+--------------+------------+-----------+--------------+--------+----------+\n",
      "|     0|    0|           0|  0|  0|    0|     0|             0|           0|          0|             0|       0|         0|\n",
      "+------+-----+------------+---+---+-----+------+--------------+------------+-----------+--------------+--------+----------+\n",
      "\n",
      "Users Missing Values:\n",
      "+-------+--------+-------+\n",
      "|user_id|products|reviews|\n",
      "+-------+--------+-------+\n",
      "|      0|       0|      0|\n",
      "+-------+--------+-------+\n",
      "\n",
      "Recommendations Missing Values:\n",
      "+------+-------+-----+----+--------------+-----+-------+---------+\n",
      "|app_id|helpful|funny|date|is_recommended|hours|user_id|review_id|\n",
      "+------+-------+-----+----+--------------+-----+-------+---------+\n",
      "|     0|      0|    0|   0|             0|    0|      0|        0|\n",
      "+------+-------+-----+----+--------------+-----+-------+---------+\n",
      "\n",
      "\n",
      "================================================================================\n",
      "DATASET STATISTICS\n",
      "================================================================================\n",
      "\n",
      "Unique Users: 13,781,059\n",
      "Unique Games: 37,610\n",
      "Possible Interactions: 518,305,628,990\n",
      "Actual Interactions: 41,154,794\n",
      "Matrix Sparsity: 99.9921%\n",
      "\n",
      "Games Statistics:\n",
      "+-------+------------------+------------------+------------------+\n",
      "|summary|       price_final|    positive_ratio|      user_reviews|\n",
      "+-------+------------------+------------------+------------------+\n",
      "|  count|             50872|             50872|             50872|\n",
      "|   mean| 8.620324933161147|  77.0520325522881|1824.4249882056927|\n",
      "| stddev|11.514164011296065|18.253592312792428|40073.521653264586|\n",
      "|    min|               0.0|                 0|                10|\n",
      "|    max|            299.99|               100|           7494460|\n",
      "+-------+------------------+------------------+------------------+\n",
      "\n",
      "\n",
      "Recommendations Statistics:\n",
      "+-------+------------------+-----------------+------------------+\n",
      "|summary|             hours|          helpful|             funny|\n",
      "+-------+------------------+-----------------+------------------+\n",
      "|  count|          41154794|         41154794|          41154794|\n",
      "|   mean|100.60223779935015|3.202566850413587|1.0580708531793404|\n",
      "| stddev|176.16754130450667|46.93648570112762|28.670602782600543|\n",
      "|    min|               0.0|                0|                 0|\n",
      "|    max|            1000.0|            36212|             28109|\n",
      "+-------+------------------+-----------------+------------------+\n",
      "\n",
      "\n",
      "✓ All datasets cached successfully\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# CODE 1: ENVIRONMENT SETUP AND DATA LOADING - CLEANED DATASET\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SteamGameRecommendationSystem\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .config(\"spark.default.parallelism\", \"200\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"512m\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STEAM GAME RECOMMENDATION SYSTEM - DATA LOADING\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nSpark Version: {spark.version}\\n\")\n",
    "\n",
    "games_df = spark.read.csv(\"games.csv\", header=True, inferSchema=True, escape='\"', multiLine=True)\n",
    "print(f\"[1/4] Games: {games_df.count():,} rows\")\n",
    "print(\"Schema:\")\n",
    "games_df.printSchema()\n",
    "print(\"\\nSample:\")\n",
    "games_df.show(3, truncate=False)\n",
    "\n",
    "users_df = spark.read.csv(\"users.csv\", header=True, inferSchema=True, escape='\"')\n",
    "print(f\"\\n[2/4] Users: {users_df.count():,} rows\")\n",
    "print(\"Schema:\")\n",
    "users_df.printSchema()\n",
    "print(\"\\nSample:\")\n",
    "users_df.show(3, truncate=False)\n",
    "\n",
    "recommendations_df = spark.read.csv(\"recommendations.csv\", header=True, inferSchema=True, escape='\"', multiLine=True)\n",
    "print(f\"\\n[3/4] Recommendations: {recommendations_df.count():,} rows\")\n",
    "print(\"Schema:\")\n",
    "recommendations_df.printSchema()\n",
    "print(\"\\nSample:\")\n",
    "recommendations_df.show(3, truncate=False)\n",
    "\n",
    "games_metadata_df = spark.read.json(\"games_metadata.json\", multiLine=True)\n",
    "print(f\"\\n[4/4] Games Metadata: {games_metadata_df.count():,} rows\")\n",
    "print(\"Schema:\")\n",
    "games_metadata_df.printSchema()\n",
    "print(\"\\nSample:\")\n",
    "games_metadata_df.show(3, truncate=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DATA QUALITY CHECK\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nGames Missing Values:\")\n",
    "games_df.select([count(when(col(c).isNull(), c)).alias(c) for c in games_df.columns]).show()\n",
    "\n",
    "print(\"Users Missing Values:\")\n",
    "users_df.select([count(when(col(c).isNull(), c)).alias(c) for c in users_df.columns]).show()\n",
    "\n",
    "print(\"Recommendations Missing Values:\")\n",
    "recommendations_df.select([count(when(col(c).isNull(), c)).alias(c) for c in recommendations_df.columns]).show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DATASET STATISTICS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "unique_users_in_rec = recommendations_df.select('user_id').distinct().count()\n",
    "unique_games_in_rec = recommendations_df.select('app_id').distinct().count()\n",
    "total_possible_interactions = unique_users_in_rec * unique_games_in_rec\n",
    "actual_interactions = recommendations_df.count()\n",
    "sparsity = (1 - (actual_interactions / total_possible_interactions)) * 100\n",
    "\n",
    "print(f\"\\nUnique Users: {unique_users_in_rec:,}\")\n",
    "print(f\"Unique Games: {unique_games_in_rec:,}\")\n",
    "print(f\"Possible Interactions: {total_possible_interactions:,}\")\n",
    "print(f\"Actual Interactions: {actual_interactions:,}\")\n",
    "print(f\"Matrix Sparsity: {sparsity:.4f}%\")\n",
    "\n",
    "print(\"\\nGames Statistics:\")\n",
    "games_df.select('price_final', 'positive_ratio', 'user_reviews').describe().show()\n",
    "\n",
    "print(\"\\nRecommendations Statistics:\")\n",
    "recommendations_df.select('hours', 'helpful', 'funny').describe().show()\n",
    "\n",
    "games_df.cache()\n",
    "users_df.cache()\n",
    "recommendations_df.cache()\n",
    "games_metadata_df.cache()\n",
    "\n",
    "print(\"\\n✓ All datasets cached successfully\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75256752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CODE 2: DATA PREPROCESSING\n",
      "================================================================================\n",
      "\n",
      "[STEP 1] Preparing recommendations...\n",
      "✓ Recommendations cleaned: 41,154,794\n",
      "+-------+-------+--------------+-----+------------------+\n",
      "|user_id| app_id|is_recommended|hours|   implicit_rating|\n",
      "+-------+-------+--------------+-----+------------------+\n",
      "|  51580| 975370|          true| 36.3|  4.61899332664977|\n",
      "|   2586| 304390|         false| 11.5|1.4742713556917444|\n",
      "| 253880|1085660|          true|336.5|               5.0|\n",
      "| 259432| 703080|          true| 27.4|  4.34638914516716|\n",
      "|  23869| 526870|          true|  7.9| 3.186051276738094|\n",
      "|  45425| 306130|          true|  8.6|3.2617630984737906|\n",
      "|  88282| 238960|          true|538.8|               5.0|\n",
      "|  63209|    730|         false|157.5|               0.0|\n",
      "| 354512| 255710|          true| 18.7|3.9806186357439426|\n",
      "| 454422| 289070|          true|397.5|               5.0|\n",
      "+-------+-------+--------------+-----+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "[STEP 2] Preparing game features...\n",
      "✓ Game features prepared: 50,872\n",
      "+------+--------------------+-----------+--------------+--------------+\n",
      "|app_id|               title|price_final|rating_encoded|platform_count|\n",
      "+------+--------------------+-----------+--------------+--------------+\n",
      "| 13500|Prince of Persia:...|       9.99|           1.0|             1|\n",
      "| 22364|BRINK: Agents of ...|       2.99|           0.0|             1|\n",
      "|113020|Monaco: What's Yo...|      14.99|           1.0|             3|\n",
      "|226560|  Escape Dead Island|      14.99|           2.0|             1|\n",
      "|249050|Dungeon of the EN...|      11.99|           1.0|             2|\n",
      "|250180|        METAL SLUG 3|       7.99|           1.0|             1|\n",
      "|253980|             Enclave|       4.99|           3.0|             3|\n",
      "|271850|Men of War: Assau...|       6.99|           2.0|             1|\n",
      "|282900|Hyperdimension Ne...|      14.99|           1.0|             1|\n",
      "| 19810|The Sum of All Fears|       9.99|           3.0|             1|\n",
      "+------+--------------------+-----------+--------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "[STEP 3] Scaling numerical features...\n",
      "✓ Features scaled\n",
      "\n",
      "[STEP 4] Creating user profiles...\n",
      "✓ User profiles created: 13,781,059\n",
      "+--------+------------------+----------------+\n",
      "| user_id|total_interactions|engagement_level|\n",
      "+--------+------------------+----------------+\n",
      "|11659871|               188|       very_high|\n",
      "| 5843184|                58|            high|\n",
      "|12507437|                 1|             low|\n",
      "| 6634874|                23|          medium|\n",
      "| 8710897|                 8|             low|\n",
      "|11325244|                 2|             low|\n",
      "| 7372093|                 5|             low|\n",
      "| 3896647|                 2|             low|\n",
      "| 6962105|                13|          medium|\n",
      "| 5708143|                 2|             low|\n",
      "+--------+------------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "[STEP 5] Creating final training dataset...\n",
      "✓ Training dataset: 41,154,794 rows\n",
      "  Unique users: 13,781,059\n",
      "  Unique games: 37,610\n",
      "root\n",
      " |-- app_id: integer (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- is_recommended: boolean (nullable = true)\n",
      " |-- hours: double (nullable = false)\n",
      " |-- helpful: integer (nullable = true)\n",
      " |-- funny: integer (nullable = true)\n",
      " |-- implicit_rating: double (nullable = false)\n",
      " |-- engagement_level: string (nullable = false)\n",
      " |-- engagement_encoded: double (nullable = false)\n",
      " |-- avg_rating_given: double (nullable = false)\n",
      " |-- total_hours_played: double (nullable = false)\n",
      " |-- title: string (nullable = true)\n",
      " |-- price_final: double (nullable = true)\n",
      " |-- rating_encoded: double (nullable = false)\n",
      " |-- platform_count: integer (nullable = true)\n",
      " |-- steam_deck_int: integer (nullable = true)\n",
      "\n",
      "+-------+-------+--------------+-----+-------+-----+------------------+----------------+------------------+------------------+------------------+-----------------------+-----------+--------------+--------------+--------------+\n",
      "|app_id |user_id|is_recommended|hours|helpful|funny|implicit_rating   |engagement_level|engagement_encoded|avg_rating_given  |total_hours_played|title                  |price_final|rating_encoded|platform_count|steam_deck_int|\n",
      "+-------+-------+--------------+-----+-------+-----+------------------+----------------+------------------+------------------+------------------+-----------------------+-----------+--------------+--------------+--------------+\n",
      "|427520 |148    |true          |155.1|0      |0    |5.0               |low             |0.0               |5.0               |155.1             |Factorio               |35.0       |5.0           |3             |1             |\n",
      "|617670 |463    |true          |9.0  |0      |0    |3.302585092994046 |low             |0.0               |2.2424533248940004|9.2               |Zup! S                 |1.99       |5.0           |1             |1             |\n",
      "|1782210|463    |true          |0.2  |0      |0    |1.1823215567939547|low             |0.0               |2.2424533248940004|9.2               |Crab Game              |0.0        |1.0           |3             |1             |\n",
      "|1343400|471    |true          |997.4|0      |0    |5.0               |low             |0.0               |5.0               |997.4             |RuneScape ®            |0.0        |1.0           |2             |1             |\n",
      "|238960 |496    |true          |109.7|0      |0    |5.0               |low             |0.0               |5.0               |109.7             |Path of Exile          |0.0        |1.0           |2             |1             |\n",
      "|223750 |833    |true          |5.1  |0      |0    |2.8082887711792655|low             |0.0               |2.8082887711792655|5.1               |DCS World Steam Edition|0.0        |1.0           |1             |1             |\n",
      "|203160 |1088   |true          |6.3  |0      |0    |2.9878743481543455|low             |0.0               |2.9878743481543455|6.3               |Tomb Raider            |2.99       |5.0           |3             |1             |\n",
      "|1222680|1238   |true          |24.9 |0      |0    |4.254242968705492 |low             |0.0               |4.254242968705492 |24.9              |Need for Speed™ Heat   |7.0        |1.0           |1             |1             |\n",
      "|601510 |1342   |false         |0.2  |0      |0    |3.817678443206045 |low             |0.0               |3.817678443206045 |0.2               |Yu-Gi-Oh! Duel Links   |0.0        |1.0           |1             |1             |\n",
      "|4000   |1580   |true          |787.9|0      |0    |5.0               |low             |0.0               |5.0               |787.9             |Garry's Mod            |10.0       |5.0           |3             |1             |\n",
      "+-------+-------+--------------+-----+-------+-----+------------------+----------------+------------------+------------------+------------------+-----------------------+-----------+--------------+--------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "[STEP 6] Reducing data for manageable ALS training...\n",
      "✓ Data reduced: 10,284,174 rows (from 41,154,794)\n",
      "  Reduction: 75.0%\n",
      "\n",
      "✓ Training data cached\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# CODE 2: DATA PREPROCESSING FOR CLEANED DATASET\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CODE 2: DATA PREPROCESSING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Step 1: Clean and prepare recommendations\n",
    "print(\"\\n[STEP 1] Preparing recommendations...\")\n",
    "recommendations_clean = recommendations_df.select(\n",
    "    'user_id',\n",
    "    'app_id',\n",
    "    'is_recommended',\n",
    "    'hours',\n",
    "    'helpful',\n",
    "    'funny'\n",
    ").fillna(0.0, subset=['hours', 'helpful', 'funny'])\n",
    "\n",
    "# Create implicit rating\n",
    "recommendations_clean = recommendations_clean.withColumn(\n",
    "    'implicit_rating',\n",
    "    when(col('is_recommended') == True,\n",
    "         when(col('hours') > 0, \n",
    "              least(log(col('hours') + 1) + 1, lit(5.0))\n",
    "         ).otherwise(lit(3.0))\n",
    "    ).otherwise(\n",
    "         when(col('hours') > 0,\n",
    "              5.0 - least(log(col('hours') + 1) + 1, lit(5.0))\n",
    "         ).otherwise(lit(2.0))\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"✓ Recommendations cleaned: {recommendations_clean.count():,}\")\n",
    "recommendations_clean.select('user_id', 'app_id', 'is_recommended', 'hours', 'implicit_rating').show(10)\n",
    "\n",
    "# Step 2: Prepare games features\n",
    "print(\"\\n[STEP 2] Preparing game features...\")\n",
    "games_features = games_df.select(\n",
    "    'app_id',\n",
    "    'title',\n",
    "    'price_final',\n",
    "    'positive_ratio',\n",
    "    'user_reviews',\n",
    "    'rating',\n",
    "    'win',\n",
    "    'mac',\n",
    "    'linux',\n",
    "    'steam_deck'\n",
    ")\n",
    "\n",
    "# Encode rating\n",
    "rating_indexer = StringIndexer(\n",
    "    inputCol='rating',\n",
    "    outputCol='rating_encoded',\n",
    "    handleInvalid='skip'\n",
    ")\n",
    "games_encoded = rating_indexer.fit(games_features).transform(games_features)\n",
    "\n",
    "# Platform features\n",
    "games_encoded = games_encoded.withColumn(\n",
    "    'platform_count',\n",
    "    (col('win').cast('int') + col('mac').cast('int') + col('linux').cast('int'))\n",
    ").withColumn(\n",
    "    'steam_deck_int',\n",
    "    col('steam_deck').cast('int')\n",
    ")\n",
    "\n",
    "print(f\"✓ Game features prepared: {games_encoded.count():,}\")\n",
    "games_encoded.select('app_id', 'title', 'price_final', 'rating_encoded', 'platform_count').show(10)\n",
    "\n",
    "# Step 3: Scale numerical features\n",
    "print(\"\\n[STEP 3] Scaling numerical features...\")\n",
    "numerical_cols = ['price_final', 'positive_ratio', 'user_reviews']\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=numerical_cols,\n",
    "    outputCol='game_features_vector',\n",
    "    handleInvalid='skip'\n",
    ")\n",
    "games_vectorized = assembler.transform(games_encoded)\n",
    "\n",
    "scaler = StandardScaler(\n",
    "    inputCol='game_features_vector',\n",
    "    outputCol='scaled_game_features',\n",
    "    withMean=True,\n",
    "    withStd=True\n",
    ")\n",
    "scaler_model = scaler.fit(games_vectorized)\n",
    "games_scaled = scaler_model.transform(games_vectorized)\n",
    "\n",
    "print(\"✓ Features scaled\")\n",
    "\n",
    "# Step 4: Create user profiles\n",
    "print(\"\\n[STEP 4] Creating user profiles...\")\n",
    "user_profiles = recommendations_clean.groupBy('user_id').agg(\n",
    "    count('*').alias('total_interactions'),\n",
    "    avg('implicit_rating').alias('avg_rating_given'),\n",
    "    sum('hours').alias('total_hours_played'),\n",
    "    sum(when(col('is_recommended') == True, 1).otherwise(0)).alias('recommended_count')\n",
    ").fillna({\n",
    "    'avg_rating_given': 0.0,\n",
    "    'total_hours_played': 0.0,\n",
    "    'recommended_count': 0\n",
    "})\n",
    "\n",
    "user_profiles = user_profiles.withColumn(\n",
    "    'engagement_level',\n",
    "    when(col('total_interactions') > 100, 'very_high')\n",
    "    .when(col('total_interactions') > 50, 'high')\n",
    "    .when(col('total_interactions') > 10, 'medium')\n",
    "    .otherwise('low')\n",
    ")\n",
    "\n",
    "# Encode engagement\n",
    "engagement_indexer = StringIndexer(\n",
    "    inputCol='engagement_level',\n",
    "    outputCol='engagement_encoded'\n",
    ")\n",
    "user_profiles = engagement_indexer.fit(user_profiles).transform(user_profiles)\n",
    "\n",
    "print(f\"✓ User profiles created: {user_profiles.count():,}\")\n",
    "user_profiles.select('user_id', 'total_interactions', 'engagement_level').show(10)\n",
    "\n",
    "# Step 5: Combine all features\n",
    "print(\"\\n[STEP 5] Creating final training dataset...\")\n",
    "training_data = recommendations_clean.join(\n",
    "    user_profiles.select('user_id', 'engagement_level', 'engagement_encoded', 'avg_rating_given', 'total_hours_played'),\n",
    "    on='user_id',\n",
    "    how='inner'\n",
    ").join(\n",
    "    games_scaled.select(\n",
    "        'app_id', 'title', 'price_final', 'rating_encoded', 'platform_count', 'steam_deck_int'\n",
    "    ),\n",
    "    on='app_id',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(f\"✓ Training dataset: {training_data.count():,} rows\")\n",
    "print(f\"  Unique users: {training_data.select('user_id').distinct().count():,}\")\n",
    "print(f\"  Unique games: {training_data.select('app_id').distinct().count():,}\")\n",
    "\n",
    "training_data.printSchema()\n",
    "training_data.show(10, truncate=False)\n",
    "\n",
    "# Step 6: Reduce data for faster ALS training\n",
    "print(\"\\n[STEP 6] Reducing data for manageable ALS training...\")\n",
    "sample_fraction = 0.25\n",
    "sampled_users = training_data.select('user_id').distinct().sample(fraction=sample_fraction, seed=42)\n",
    "training_data_reduced = training_data.join(sampled_users, on='user_id', how='inner')\n",
    "\n",
    "reduced_count = training_data_reduced.count()\n",
    "print(f\"✓ Data reduced: {reduced_count:,} rows (from {training_data.count():,})\")\n",
    "print(f\"  Reduction: {(1 - reduced_count/training_data.count())*100:.1f}%\")\n",
    "\n",
    "# Cache and use reduced data\n",
    "training_data = training_data_reduced\n",
    "training_data.cache()\n",
    "training_data.count()\n",
    "\n",
    "print(\"\\n✓ Training data cached\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "229e9fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CODE 3: ALS COLLABORATIVE FILTERING\n",
      "================================================================================\n",
      "ALS samples: 9,896,021\n",
      "+-------+-----------------+-----------------+------------------+\n",
      "|summary|             user|             item|            rating|\n",
      "+-------+-----------------+-----------------+------------------+\n",
      "|  count|          9896021|          9896021|           9896021|\n",
      "|   mean|7448383.135804987|601505.5614731415|3.8570932689966178|\n",
      "| stddev|4016469.412926007|471804.4857823358|1.2242009086798202|\n",
      "|    min|                2|               10|      0.0017992983|\n",
      "|    max|         14306062|          2245890|               5.0|\n",
      "+-------+-----------------+-----------------+------------------+\n",
      "\n",
      "\n",
      "Training ALS...\n",
      "✓ ALS RMSE: 1.5425\n",
      "✓ Recommendations for 2,947,250 users\n",
      "+----+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|user|recommendations                                                                                                                                                                                                       |\n",
      "+----+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|1342|[{1076640, 6.167912}, {1970820, 5.0221734}, {528640, 4.624461}, {393340, 4.4830747}, {2106450, 4.330432}, {1244390, 4.3053856}, {1453850, 4.2748795}, {1509720, 4.1982417}, {1215000, 4.089455}, {1053170, 4.0552087}]|\n",
      "|4935|[{1066110, 4.4198503}, {1076640, 4.3868904}, {1244390, 3.955493}, {1970820, 3.8880968}, {1943740, 3.8703883}, {776920, 3.8598814}, {1394830, 3.5266929}, {914950, 3.5145164}, {38150, 3.4953096}, {957720, 3.406652}] |\n",
      "|7253|[{1150500, 7.366925}, {1557690, 7.218325}, {1394830, 7.0961094}, {729310, 6.9563766}, {1745710, 6.9171534}, {1385260, 6.769045}, {1281170, 6.630171}, {802210, 6.529966}, {1791460, 6.4455876}, {1530670, 6.413599}]  |\n",
      "+----+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ALSModel' object has no attribute 'cache'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 50>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✓ Recommendations for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_recs\u001b[38;5;241m.\u001b[39mcount()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m users\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     48\u001b[0m user_recs\u001b[38;5;241m.\u001b[39mlimit(\u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecommendations\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m3\u001b[39m, truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 50\u001b[0m \u001b[43mals_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache\u001b[49m()\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✓ ALS model cached\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m80\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ALSModel' object has no attribute 'cache'"
     ]
    }
   ],
   "source": [
    "# CODE 3: ALS COLLABORATIVE FILTERING - FAST\n",
    "\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CODE 3: ALS COLLABORATIVE FILTERING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "training_data_als = training_data.select(\n",
    "    col('user_id').cast('int').alias('user'),\n",
    "    col('app_id').cast('int').alias('item'),\n",
    "    col('implicit_rating').cast('float').alias('rating')\n",
    ").filter(col('rating') > 0.0).repartition(100)\n",
    "\n",
    "print(f\"ALS samples: {training_data_als.count():,}\")\n",
    "training_data_als.select('user', 'item', 'rating').describe().show()\n",
    "\n",
    "train_als, test_als = training_data_als.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "als = ALS(\n",
    "    userCol='user',\n",
    "    itemCol='item',\n",
    "    ratingCol='rating',\n",
    "    maxIter=15,\n",
    "    rank=20,\n",
    "    regParam=0.05,\n",
    "    alpha=10.0,\n",
    "    coldStartStrategy='drop',\n",
    "    nonnegative=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"\\nTraining ALS...\")\n",
    "als_model = als.fit(train_als)\n",
    "\n",
    "als_predictions = als_model.transform(test_als)\n",
    "als_valid_preds = als_predictions.filter(col('prediction').isNotNull())\n",
    "\n",
    "evaluator = RegressionEvaluator(metricName='rmse', labelCol='rating', predictionCol='prediction')\n",
    "als_rmse = evaluator.evaluate(als_valid_preds)\n",
    "\n",
    "print(f\"✓ ALS RMSE: {als_rmse:.4f}\")\n",
    "\n",
    "user_recs = als_model.recommendForAllUsers(10)\n",
    "print(f\"✓ Recommendations for {user_recs.count():,} users\")\n",
    "user_recs.limit(3).select('user', 'recommendations').show(3, truncate=False)\n",
    "\n",
    "als_model.cache()\n",
    "print(\"✓ ALS model cached\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1f35753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking data quality before ALS:\n",
      "Total training records: 10,284,174\n",
      "Null implicit_rating: 0\n",
      "Zero/negative ratings: 388,153\n",
      "Valid ratings: 9,896,021\n",
      "\n",
      "Rating distribution:\n",
      "+-------+--------------------+\n",
      "|summary|     implicit_rating|\n",
      "+-------+--------------------+\n",
      "|  count|             9896021|\n",
      "|   mean|  3.8570932650295293|\n",
      "| stddev|  1.2242009099016957|\n",
      "|    min|0.001799298330801...|\n",
      "|    max|                 5.0|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DIAGNOSTIC CODE\n",
    "print(\"Checking data quality before ALS:\")\n",
    "print(f\"Total training records: {training_data.count():,}\")\n",
    "print(f\"Null implicit_rating: {training_data.filter(col('implicit_rating').isNull()).count():,}\")\n",
    "print(f\"Zero/negative ratings: {training_data.filter(col('implicit_rating') <= 0).count():,}\")\n",
    "print(f\"Valid ratings: {training_data.filter(col('implicit_rating') > 0).count():,}\")\n",
    "\n",
    "print(\"\\nRating distribution:\")\n",
    "training_data.filter(col('implicit_rating') > 0).select('implicit_rating').describe().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba703b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CODE 4: GRADIENT BOOSTING - CONTENT-BASED FILTERING\n",
      "================================================================================\n",
      "\n",
      "Using features: ['price_final', 'rating_encoded', 'platform_count', 'steam_deck_int', 'engagement_encoded']\n",
      "Training samples for GBT: 10,284,174\n",
      "Train: 8,225,831, Test: 2,058,343\n",
      "\n",
      "Training Gradient Boosting...\n",
      "✓ Gradient Boosting RMSE: 1.3231\n",
      "\n",
      "Feature Importance:\n",
      "  price_final: 0.5172\n",
      "  rating_encoded: 0.2820\n",
      "  platform_count: 0.0903\n",
      "  steam_deck_int: 0.0004\n",
      "  engagement_encoded: 0.1101\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'GBTRegressionModel' object has no attribute 'cache'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 73>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m importance \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     71\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeature_cols[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimportance\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 73\u001b[0m \u001b[43mgbt_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbestModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache\u001b[49m()\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m✓ GBT model cached\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m80\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'GBTRegressionModel' object has no attribute 'cache'"
     ]
    }
   ],
   "source": [
    "# CODE 4: CONTENT-BASED FILTERING WITH GRADIENT BOOSTING\n",
    "\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CODE 4: GRADIENT BOOSTING - CONTENT-BASED FILTERING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Features that actually exist in training_data\n",
    "feature_cols = ['price_final', 'rating_encoded', 'platform_count', 'steam_deck_int', 'engagement_encoded']\n",
    "\n",
    "print(f\"\\nUsing features: {feature_cols}\")\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol='features',\n",
    "    handleInvalid='skip'\n",
    ")\n",
    "\n",
    "training_features = assembler.transform(training_data.fillna(0)).dropna(subset=['features'])\n",
    "\n",
    "print(f\"Training samples for GBT: {training_features.count():,}\")\n",
    "\n",
    "train_data, test_data = training_features.randomSplit([0.8, 0.2], seed=42)\n",
    "print(f\"Train: {train_data.count():,}, Test: {test_data.count():,}\")\n",
    "\n",
    "gbt = GBTRegressor(\n",
    "    featuresCol='features',\n",
    "    labelCol='implicit_rating',\n",
    "    maxDepth=5,\n",
    "    maxBins=32,\n",
    "    minInstancesPerNode=1,\n",
    "    subsamplingRate=0.8,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "param_grid_gbt = ParamGridBuilder() \\\n",
    "    .addGrid(gbt.maxDepth, [5, 7]) \\\n",
    "    .addGrid(gbt.maxBins, [32, 64]) \\\n",
    "    .build()\n",
    "\n",
    "evaluator_gbt = RegressionEvaluator(\n",
    "    metricName='rmse',\n",
    "    labelCol='implicit_rating',\n",
    "    predictionCol='prediction'\n",
    ")\n",
    "\n",
    "tvs_gbt = TrainValidationSplit(\n",
    "    estimator=gbt,\n",
    "    estimatorParamMaps=param_grid_gbt,\n",
    "    evaluator=evaluator_gbt,\n",
    "    trainRatio=0.8,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"\\nTraining Gradient Boosting...\")\n",
    "gbt_model = tvs_gbt.fit(train_data)\n",
    "\n",
    "gbt_predictions = gbt_model.transform(test_data)\n",
    "gbt_rmse = evaluator_gbt.evaluate(gbt_predictions)\n",
    "print(f\"✓ Gradient Boosting RMSE: {gbt_rmse:.4f}\")\n",
    "\n",
    "print(\"\\nFeature Importance:\")\n",
    "feature_importance = gbt_model.bestModel.featureImportances\n",
    "for i, importance in enumerate(feature_importance):\n",
    "    if importance > 0:\n",
    "        print(f\"  {feature_cols[i]}: {importance:.4f}\")\n",
    "\n",
    "gbt_model.bestModel.cache()\n",
    "print(\"\\n✓ GBT model cached\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a450d4f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CODE 5: RANDOM FOREST - CONTENT-BASED FILTERING\n",
      "================================================================================\n",
      "\n",
      "[STEP 1] Reducing training data for faster RF training...\n",
      "Original size: 10,284,174\n",
      "Target size: 1,000,000\n",
      "Reduced size: 1,000,928\n",
      "Reduction: 90.3%\n",
      "\n",
      "[STEP 2] Creating feature vectors...\n",
      "Features created: 1,000,928\n",
      "Train: 800,728, Test: 200,200\n",
      "\n",
      "[STEP 3] Training Random Forest...\n",
      "✓ Random Forest RMSE: 1.3326\n",
      "\n",
      "Feature Importance:\n",
      "  price_final: 0.4631\n",
      "  rating_encoded: 0.3319\n",
      "  platform_count: 0.0328\n",
      "  steam_deck_int: 0.0001\n",
      "  engagement_encoded: 0.1720\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'RandomForestRegressionModel' object has no attribute 'cache'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 88>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m importance \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     86\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeature_cols[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimportance\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 88\u001b[0m \u001b[43mrf_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbestModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache\u001b[49m()\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m✓ RF model cached\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m80\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'RandomForestRegressionModel' object has no attribute 'cache'"
     ]
    }
   ],
   "source": [
    "# CODE 5: RANDOM FOREST FOR RATING PREDICTION - WITH DATA REDUCTION\n",
    "\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CODE 5: RANDOM FOREST - CONTENT-BASED FILTERING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Reduce training data to ~1 million (10 lakh) for faster RF training\n",
    "print(\"\\n[STEP 1] Reducing training data for faster RF training...\")\n",
    "current_size = training_data.count()\n",
    "target_size = 1000000  # 10 lakh\n",
    "sample_fraction = target_size / current_size\n",
    "\n",
    "training_data_rf_reduced = training_data.sample(fraction=sample_fraction, seed=42)\n",
    "reduced_size = training_data_rf_reduced.count()\n",
    "\n",
    "print(f\"Original size: {current_size:,}\")\n",
    "print(f\"Target size: {target_size:,}\")\n",
    "print(f\"Reduced size: {reduced_size:,}\")\n",
    "print(f\"Reduction: {(1 - reduced_size/current_size)*100:.1f}%\")\n",
    "\n",
    "# Create features\n",
    "print(\"\\n[STEP 2] Creating feature vectors...\")\n",
    "feature_cols = ['price_final', 'rating_encoded', 'platform_count', 'steam_deck_int', 'engagement_encoded']\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol='features',\n",
    "    handleInvalid='skip'\n",
    ")\n",
    "\n",
    "training_features = assembler.transform(training_data_rf_reduced.fillna(0)).dropna(subset=['features'])\n",
    "\n",
    "print(f\"Features created: {training_features.count():,}\")\n",
    "\n",
    "# Split data\n",
    "train_data_rf, test_data_rf = training_features.randomSplit([0.8, 0.2], seed=42)\n",
    "print(f\"Train: {train_data_rf.count():,}, Test: {test_data_rf.count():,}\")\n",
    "\n",
    "# Random Forest model\n",
    "rf = RandomForestRegressor(\n",
    "    featuresCol='features',\n",
    "    labelCol='implicit_rating',\n",
    "    numTrees=50,\n",
    "    maxDepth=10,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Hyperparameter grid\n",
    "param_grid_rf = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [50, 100]) \\\n",
    "    .addGrid(rf.maxDepth, [8, 10]) \\\n",
    "    .build()\n",
    "\n",
    "evaluator_rf = RegressionEvaluator(\n",
    "    metricName='rmse',\n",
    "    labelCol='implicit_rating',\n",
    "    predictionCol='prediction'\n",
    ")\n",
    "\n",
    "tvs_rf = TrainValidationSplit(\n",
    "    estimator=rf,\n",
    "    estimatorParamMaps=param_grid_rf,\n",
    "    evaluator=evaluator_rf,\n",
    "    trainRatio=0.8,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"\\n[STEP 3] Training Random Forest...\")\n",
    "rf_model = tvs_rf.fit(train_data_rf)\n",
    "\n",
    "# Evaluate\n",
    "rf_predictions = rf_model.transform(test_data_rf)\n",
    "rf_rmse = evaluator_rf.evaluate(rf_predictions)\n",
    "print(f\"✓ Random Forest RMSE: {rf_rmse:.4f}\")\n",
    "\n",
    "print(\"\\nFeature Importance:\")\n",
    "feature_importance_rf = rf_model.bestModel.featureImportances\n",
    "for i, importance in enumerate(feature_importance_rf):\n",
    "    if importance > 0:\n",
    "        print(f\"  {feature_cols[i]}: {importance:.4f}\")\n",
    "\n",
    "rf_model.bestModel.cache()\n",
    "print(\"\\n✓ RF model cached\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b41304a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CODE 6: STACKING ENSEMBLE\n",
      "================================================================================\n",
      "\n",
      "Generating base model predictions...\n",
      "Ensemble training samples: 26,449\n",
      "Training meta-learner...\n",
      "✓ Ensemble RMSE: 1.3123\n",
      "\n",
      "Meta-learner weights:\n",
      "  GBT coefficient: 0.9359\n",
      "  RF coefficient: 0.0220\n",
      "  Intercept: 0.1359\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LinearRegressionModel' object has no attribute 'cache'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 60>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  RF coefficient: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mensemble_model\u001b[38;5;241m.\u001b[39mcoefficients[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Intercept: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mensemble_model\u001b[38;5;241m.\u001b[39mintercept\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 60\u001b[0m \u001b[43mensemble_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache\u001b[49m()\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m✓ Ensemble model cached\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m80\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'LinearRegressionModel' object has no attribute 'cache'"
     ]
    }
   ],
   "source": [
    "# CODE 6: STACKING ENSEMBLE MODEL\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CODE 6: STACKING ENSEMBLE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get predictions from both models\n",
    "print(\"\\nGenerating base model predictions...\")\n",
    "gbt_preds = gbt_model.bestModel.transform(test_data).select(\n",
    "    'user_id', 'app_id', 'implicit_rating', col('prediction').alias('gbt_pred')\n",
    ")\n",
    "\n",
    "rf_preds = rf_model.bestModel.transform(test_data_rf).select(\n",
    "    'user_id', 'app_id', col('prediction').alias('rf_pred')\n",
    ")\n",
    "\n",
    "# Combine predictions\n",
    "ensemble_data = gbt_preds.join(\n",
    "    rf_preds.select('user_id', 'app_id', 'rf_pred'),\n",
    "    on=['user_id', 'app_id'],\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(f\"Ensemble training samples: {ensemble_data.count():,}\")\n",
    "\n",
    "# Meta-learner features\n",
    "meta_assembler = VectorAssembler(\n",
    "    inputCols=['gbt_pred', 'rf_pred'],\n",
    "    outputCol='meta_features'\n",
    ")\n",
    "meta_data = meta_assembler.transform(ensemble_data)\n",
    "\n",
    "# Train meta-learner\n",
    "meta_learner = LinearRegression(\n",
    "    featuresCol='meta_features',\n",
    "    labelCol='implicit_rating',\n",
    "    maxIter=100\n",
    ")\n",
    "\n",
    "print(\"Training meta-learner...\")\n",
    "ensemble_model = meta_learner.fit(meta_data)\n",
    "\n",
    "# Evaluate ensemble\n",
    "ensemble_preds = ensemble_model.transform(meta_data)\n",
    "evaluator_ensemble = RegressionEvaluator(\n",
    "    metricName='rmse',\n",
    "    labelCol='implicit_rating',\n",
    "    predictionCol='prediction'\n",
    ")\n",
    "ensemble_rmse = evaluator_ensemble.evaluate(ensemble_preds)\n",
    "\n",
    "print(f\"✓ Ensemble RMSE: {ensemble_rmse:.4f}\")\n",
    "print(f\"\\nMeta-learner weights:\")\n",
    "print(f\"  GBT coefficient: {ensemble_model.coefficients[0]:.4f}\")\n",
    "print(f\"  RF coefficient: {ensemble_model.coefficients[1]:.4f}\")\n",
    "print(f\"  Intercept: {ensemble_model.intercept:.4f}\")\n",
    "\n",
    "ensemble_model.cache()\n",
    "print(\"\\n✓ Ensemble model cached\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4c232b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CODE 7: MODEL EXPLAINABILITY (XAI)\n",
      "================================================================================\n",
      "\n",
      "Analyzing 1000 predictions...\n",
      "\n",
      "Prediction Error Statistics:\n",
      "        prediction  implicit_rating        error    abs_error\n",
      "count  1000.000000      1000.000000  1000.000000  1000.000000\n",
      "mean      3.670801         3.655151     0.015649     1.056508\n",
      "std       0.440082         1.398482     1.321994     0.794094\n",
      "min       2.431049         0.000000    -2.430318     0.000740\n",
      "25%       3.378209         2.791759    -0.933366     0.523196\n",
      "50%       3.721290         3.949675    -0.362464     0.888497\n",
      "75%       4.037504         5.000000     0.756439     1.347650\n",
      "max       4.733809         5.000000     4.510799     4.510799\n",
      "\n",
      "Error Distribution:\n",
      "  Mean Error: 0.0156\n",
      "  Std Dev: 1.3220\n",
      "  Max Error: 4.5108\n",
      "  Min Error: -2.4303\n",
      "  Median Abs Error: 0.8885\n",
      "\n",
      "High Error Cases (Top 25%, threshold: 1.3477):\n",
      "  Count: 250\n",
      "    gbt_pred   rf_pred  prediction  implicit_rating     error\n",
      "7   3.971173  3.923575    3.939050         2.410987  1.528063\n",
      "16  4.478852  4.310445    4.422718         2.518395  1.904323\n",
      "19  3.127085  3.214635    3.133436         5.000000 -1.866564\n",
      "24  3.822788  3.789465    3.797220         0.000000  3.797220\n",
      "42  2.698622  2.841608    2.724213         4.303217 -1.579004\n",
      "48  3.545266  3.628146    3.533927         5.000000 -1.466073\n",
      "51  2.877552  2.968099    2.894463         5.000000 -2.105537\n",
      "55  3.829310  3.807234    3.803715         2.039905  1.763810\n",
      "56  3.661158  3.657761    3.643046         1.530628  2.112417\n",
      "58  3.391260  3.344931    3.383554         4.960813 -1.577259\n",
      "\n",
      "Average Model Contributions:\n",
      "  GBT avg contribution: 1.8451\n",
      "  RF avg contribution: 1.8404\n",
      "\n",
      "✓ Explainability analysis complete\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# CODE 7: EXPLAINABILITY & XAI\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CODE 7: MODEL EXPLAINABILITY (XAI)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get sample predictions\n",
    "sample_size = 1000\n",
    "# Get sample predictions\n",
    "predictions_sample = ensemble_preds.select(\n",
    "    'gbt_pred', 'rf_pred', 'prediction', 'implicit_rating'\n",
    ").limit(sample_size).toPandas()\n",
    "\n",
    "print(f\"\\nAnalyzing {len(predictions_sample)} predictions...\")\n",
    "\n",
    "# Calculate errors correctly with Pandas abs()\n",
    "predictions_sample['error'] = predictions_sample['prediction'] - predictions_sample['implicit_rating']\n",
    "predictions_sample['abs_error'] = predictions_sample['error'].abs()\n",
    "predictions_sample['percentage_error'] = (predictions_sample['abs_error'] / (predictions_sample['implicit_rating'].abs() + 0.1)) * 100\n",
    "\n",
    "print(\"\\nPrediction Error Statistics:\")\n",
    "print(predictions_sample[['prediction', 'implicit_rating', 'error', 'abs_error']].describe())\n",
    "\n",
    "# The rest remains unchanged...\n",
    "\n",
    "\n",
    "print(\"\\nError Distribution:\")\n",
    "print(f\"  Mean Error: {predictions_sample['error'].mean():.4f}\")\n",
    "print(f\"  Std Dev: {predictions_sample['error'].std():.4f}\")\n",
    "print(f\"  Max Error: {predictions_sample['error'].max():.4f}\")\n",
    "print(f\"  Min Error: {predictions_sample['error'].min():.4f}\")\n",
    "print(f\"  Median Abs Error: {predictions_sample['abs_error'].median():.4f}\")\n",
    "\n",
    "# High error cases\n",
    "high_error_threshold = predictions_sample['abs_error'].quantile(0.75)\n",
    "high_errors = predictions_sample[predictions_sample['abs_error'] > high_error_threshold]\n",
    "\n",
    "print(f\"\\nHigh Error Cases (Top 25%, threshold: {high_error_threshold:.4f}):\")\n",
    "print(f\"  Count: {len(high_errors)}\")\n",
    "print(high_errors[['gbt_pred', 'rf_pred', 'prediction', 'implicit_rating', 'error']].head(10))\n",
    "\n",
    "# Model contribution analysis\n",
    "predictions_sample['gbt_weight'] = 0.5\n",
    "predictions_sample['rf_weight'] = 0.5\n",
    "predictions_sample['gbt_contribution'] = predictions_sample['gbt_pred'] * predictions_sample['gbt_weight']\n",
    "predictions_sample['rf_contribution'] = predictions_sample['rf_pred'] * predictions_sample['rf_weight']\n",
    "\n",
    "print(f\"\\nAverage Model Contributions:\")\n",
    "print(f\"  GBT avg contribution: {predictions_sample['gbt_contribution'].mean():.4f}\")\n",
    "print(f\"  RF avg contribution: {predictions_sample['rf_contribution'].mean():.4f}\")\n",
    "\n",
    "print(\"\\n✓ Explainability analysis complete\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d0535a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CODE 8: MODEL COMPARISON & FINAL RECOMMENDATIONS\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "MODEL PERFORMANCE COMPARISON\n",
      "============================================================\n",
      "1. Stacking Ensemble         RMSE: 1.3123  (Improvement: 14.92%)\n",
      "2. Gradient Boosting         RMSE: 1.3231  (Improvement: 14.22%)\n",
      "3. Random Forest             RMSE: 1.3326  (Improvement: 13.61%)\n",
      "4. ALS                       RMSE: 1.5425  (Improvement: 0.00%)\n",
      "\n",
      "✓ Best Model: Stacking Ensemble\n",
      "\n",
      "============================================================\n",
      "GENERATING FINAL RECOMMENDATIONS\n",
      "============================================================\n",
      "\n",
      "Recommendations with predicted rating > 3.5: 7,054,384\n",
      "\n",
      "Final recommendations with titles: 6,675,817\n",
      "\n",
      "Sample Top Game Recommendations:\n",
      "+-------+-------------------------------------+------------------+----+\n",
      "|user_id|game_title                           |predicted_rating  |rank|\n",
      "+-------+-------------------------------------+------------------+----+\n",
      "|2      |Subnautica: Below Zero               |4.299773105641976 |1   |\n",
      "|2      |GTFO                                 |4.037504495999813 |2   |\n",
      "|2      |Among Us                             |3.8260133887510923|3   |\n",
      "|2      |Brawlhalla                           |3.721421679193312 |4   |\n",
      "|7      |Insurgency: Sandstorm                |3.8403264966521315|1   |\n",
      "|11     |Frostpunk                            |4.299773105641976 |1   |\n",
      "|11     |They Are Billions                    |3.8634665310405607|2   |\n",
      "|11     |Sid Meier’s Civilization® VI         |3.841379577206954 |3   |\n",
      "|16     |SCP: Secret Laboratory               |3.6430456630361125|1   |\n",
      "|19     |STEINS;GATE 0                        |4.22338132638925  |1   |\n",
      "|21     |DOOM Eternal                         |4.037504495999813 |1   |\n",
      "|36     |PowerWash Simulator                  |4.24145049587356  |1   |\n",
      "|36     |God of War                           |4.21421079587879  |2   |\n",
      "|36     |Papers Please                        |4.075611704137586 |3   |\n",
      "|36     |Metro Exodus                         |3.944068068884524 |4   |\n",
      "|36     |Red Dead Redemption 2                |3.8698045928487117|5   |\n",
      "|36     |BIGFOOT                              |3.8698045928487117|6   |\n",
      "|36     |STAR WARS Jedi: Fallen Order™        |3.8639043855589397|7   |\n",
      "|36     |Zombie Army Trilogy                  |3.837365548742436 |8   |\n",
      "|36     |S.T.A.L.K.E.R.: Shadow of Chernobyl  |3.6297594058973006|9   |\n",
      "|36     |Dead Space (2008)                    |3.6297594058973006|10  |\n",
      "|38     |Dead Cells                           |4.510798938189322 |1   |\n",
      "|38     |Vampire Survivors                    |4.219752824866685 |2   |\n",
      "|61     |Detroit: Become Human                |4.037504495999813 |1   |\n",
      "|63     |Sid Meier’s Civilization® VI         |3.841379577206954 |1   |\n",
      "|65     |Cities: Skylines                     |4.157893420748895 |1   |\n",
      "|67     |Danganronpa 2: Goodbye Despair       |4.043599900039653 |1   |\n",
      "|68     |The Backrooms Game FREE Edition      |3.721421679193312 |1   |\n",
      "|71     |Marauders                            |3.841824846831259 |1   |\n",
      "|95     |The Witcher® 3: Wild Hunt            |4.280639596317379 |1   |\n",
      "|95     |Descenders                           |4.264581029217894 |2   |\n",
      "|95     |Metro Exodus                         |4.157893420748895 |3   |\n",
      "|95     |NieR:Automata™                       |4.037504495999813 |4   |\n",
      "|95     |Prey                                 |3.8634665310405607|5   |\n",
      "|97     |Euro Truck Simulator 2               |4.387257385964782 |1   |\n",
      "|97     |Transport Fever 2                    |4.312570894851691 |2   |\n",
      "|97     |Cities: Skylines                     |4.157893420748895 |3   |\n",
      "|97     |City Bus Manager                     |3.920436726778523 |4   |\n",
      "|97     |OMSI 2: Steam Edition                |3.8634665310405607|5   |\n",
      "|101    |American Truck Simulator             |4.387257385964782 |1   |\n",
      "|101    |NASCAR Heat 4                        |3.5689497121940055|2   |\n",
      "|103    |The Crew™ 2                          |4.127554512490107 |1   |\n",
      "|103    |Jurassic World Evolution 2           |4.067845397487563 |2   |\n",
      "|103    |Jurassic World Evolution             |4.027007602572681 |3   |\n",
      "|107    |Lost Lands: A Hidden Object Adventure|3.721421679193312 |1   |\n",
      "|113    |Half-Life 2                          |3.759982694036052 |1   |\n",
      "|116    |Grim Dawn                            |4.182484299466121 |1   |\n",
      "|118    |Batman™: Arkham Knight               |4.113431752406461 |1   |\n",
      "|121    |Celeste                              |4.387257385964782 |1   |\n",
      "|121    |Left 4 Dead 2                        |4.2888482537708965|2   |\n",
      "+-------+-------------------------------------+------------------+----+\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o4389.parquet.\n: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n\tat org.apache.parquet.hadoop.ParquetOutputCommitter.commitJob(ParquetOutputCommitter.java:48)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:390)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:418)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:390)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\r\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:802)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 96>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     91\u001b[0m final_recommendations\u001b[38;5;241m.\u001b[39mselect(\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgame_title\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredicted_rating\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrank\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     93\u001b[0m )\u001b[38;5;241m.\u001b[39morderBy(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrank\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mlimit(\u001b[38;5;241m50\u001b[39m)\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m50\u001b[39m, truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# Save recommendations for downstream use\u001b[39;00m\n\u001b[1;32m---> 96\u001b[0m \u001b[43mfinal_recommendations\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoalesce\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msteam_recommendations_final\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m✓ Recommendations saved to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msteam_recommendations_final\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m directory\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# Print summary stats\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pyspark\\sql\\readwriter.py:1721\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[1;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[0;32m   1719\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartitionBy(partitionBy)\n\u001b[0;32m   1720\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression)\n\u001b[1;32m-> 1721\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o4389.parquet.\n: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1249)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1454)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n\tat org.apache.parquet.hadoop.ParquetOutputCommitter.commitJob(ParquetOutputCommitter.java:48)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:390)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:418)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:390)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\r\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:802)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\n"
     ]
    }
   ],
   "source": [
    "# CODE 8: MODEL COMPARISON & FINAL RECOMMENDATIONS\n",
    "\n",
    "from pyspark.sql.functions import desc, row_number, col\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CODE 8: MODEL COMPARISON & FINAL RECOMMENDATIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Model comparison dictionary (fill with your actual RMSE values)\n",
    "model_comparison = {\n",
    "    'ALS': als_rmse,\n",
    "    'Gradient Boosting': gbt_rmse,\n",
    "    'Random Forest': rf_rmse,\n",
    "    'Stacking Ensemble': ensemble_rmse\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MODEL PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Convert values to list for max()\n",
    "# Use built-in max (ensure pyspark.sql.functions.max is not imported)\n",
    "max_rmse = __builtins__.max(list(model_comparison.values()))\n",
    "\n",
    "for i, (model, rmse) in enumerate(sorted(model_comparison.items(), key=lambda x: x[1]), 1):\n",
    "    improvement = ((max_rmse - rmse) / max_rmse) * 100\n",
    "    print(f\"{i}. {model:25} RMSE: {rmse:.4f}  (Improvement: {improvement:.2f}%)\")\n",
    "\n",
    "\n",
    "best_model_name = __builtins__.min(model_comparison, key=model_comparison.get)\n",
    "print(f\"\\n✓ Best Model: {best_model_name}\")\n",
    "\n",
    "# Generate final recommendations from ensemble\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"GENERATING FINAL RECOMMENDATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Features to use for prediction\n",
    "feature_cols = ['price_final', 'rating_encoded', 'platform_count', 'steam_deck_int', 'engagement_encoded']\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol='features', handleInvalid='skip')\n",
    "\n",
    "all_features = assembler.transform(training_data.fillna(0)).dropna(subset=['features'])\n",
    "\n",
    "# Get predictions from GBT and RF models\n",
    "gbt_all_preds = gbt_model.bestModel.transform(all_features).select(\n",
    "    'user_id', 'app_id', 'implicit_rating', col('prediction').alias('gbt_pred')\n",
    ")\n",
    "\n",
    "rf_all_preds = rf_model.bestModel.transform(all_features).select(\n",
    "    'user_id', 'app_id', col('prediction').alias('rf_pred')\n",
    ")\n",
    "\n",
    "# Join predictions to prepare for ensemble\n",
    "ensemble_all = gbt_all_preds.join(\n",
    "    rf_all_preds.select('user_id', 'app_id', 'rf_pred'),\n",
    "    on=['user_id', 'app_id'],\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "meta_assembler = VectorAssembler(inputCols=['gbt_pred', 'rf_pred'], outputCol='meta_features')\n",
    "meta_all = meta_assembler.transform(ensemble_all)\n",
    "\n",
    "final_preds = ensemble_model.transform(meta_all)\n",
    "\n",
    "# Filter top recommendations with predicted rating threshold\n",
    "recs = final_preds.select(\n",
    "    col('user_id').cast('int'),\n",
    "    col('app_id').cast('int'),\n",
    "    col('prediction').alias('predicted_rating')\n",
    ").filter(col('predicted_rating') > 3.5)\n",
    "\n",
    "print(f\"\\nRecommendations with predicted rating > 3.5: {recs.count():,}\")\n",
    "\n",
    "# Rank top 10 recommendations per user\n",
    "window_spec = Window.partitionBy('user_id').orderBy(desc('predicted_rating'))\n",
    "top_recs = recs.withColumn('rank', row_number().over(window_spec)) \\\n",
    "    .filter(col('rank') <= 10)\n",
    "\n",
    "# Join with game titles for display\n",
    "final_recommendations = top_recs.join(\n",
    "    games_df.select('app_id', col('title').alias('game_title')),\n",
    "    on='app_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\"\\nFinal recommendations with titles: {final_recommendations.count():,}\")\n",
    "print(\"\\nSample Top Game Recommendations:\")\n",
    "final_recommendations.select(\n",
    "    'user_id', 'game_title', 'predicted_rating', 'rank'\n",
    ").orderBy('user_id', 'rank').limit(50).show(50, truncate=False)\n",
    "\n",
    "# Save recommendations for downstream use\n",
    "# final_recommendations.coalesce(1).write.mode('overwrite').parquet('steam_recommendations_final')\n",
    "# print(\"\\n✓ Recommendations saved to 'steam_recommendations_final' directory\")\n",
    "\n",
    "# # Print summary stats\n",
    "# print(\"\\n\" + \"=\" * 60)\n",
    "# print(\"RECOMMENDATION SYSTEM SUMMARY\")\n",
    "# print(\"=\" * 60)\n",
    "# print(f\"Total Unique Users: {training_data.select('user_id').distinct().count():,}\")\n",
    "# print(f\"Total Unique Games: {training_data.select('app_id').distinct().count():,}\")\n",
    "# print(f\"Training Interactions: {training_data.count():,}\")\n",
    "# print(f\"Final Recommendations Generated: {final_recommendations.count():,}\")\n",
    "# print(f\"Best Model: {best_model_name} (RMSE: {model_comparison[best_model_name]:.4f})\")\n",
    "# print(\"\\n✓ Recommendation System Complete!\")\n",
    "# print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af8b806e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RECOMMENDATION SYSTEM SUMMARY\n",
      "============================================================\n",
      "Total Unique Users: 3,445,406\n",
      "Total Unique Games: 35,837\n",
      "Training Interactions: 10,284,174\n",
      "Final Recommendations Generated: 6,675,817\n",
      "Best Model: Stacking Ensemble (RMSE: 1.3123)\n",
      "\n",
      "✓ Recommendation System Complete!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Print summary stats\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RECOMMENDATION SYSTEM SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total Unique Users: {training_data.select('user_id').distinct().count():,}\")\n",
    "print(f\"Total Unique Games: {training_data.select('app_id').distinct().count():,}\")\n",
    "print(f\"Training Interactions: {training_data.count():,}\")\n",
    "print(f\"Final Recommendations Generated: {final_recommendations.count():,}\")\n",
    "print(f\"Best Model: {best_model_name} (RMSE: {model_comparison[best_model_name]:.4f})\")\n",
    "print(\"\\n✓ Recommendation System Complete!\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
